{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66d7773-0e61-4f6a-9005-6422684826cf",
   "metadata": {},
   "source": [
    "# Creating test cases\n",
    "This notebook takes a folder of notebooks and turns them into a jsonl file in the format human_eval expects.\n",
    "\n",
    "The notebooks have to have the following format:\n",
    "* Within one cell there must be a function that solves a specific [bio-image analysis] task.\n",
    "* This function must have a meaningful docstring between \"\"\" and \"\"\". It must be so meaningful that a language model could possibly write the entire function.\n",
    "* There must be another code cell that starts with `def check(candiate):` and contains test code to test the generated code.\n",
    "* The text code must use `assert` statements and call the `candidate` function. E.g. if a given function to test is `sum`, then a valid test for `sum` would be:\n",
    "```\n",
    "def check(candidate):\n",
    "    assert candidate(3, 4) == 7\n",
    "```\n",
    "* A third python code cell in the notebook must call the `check` function with your custom function, e.g. like this, to prove that the code you provided works with the tests you wrote:\n",
    "```\n",
    "check(sum)\n",
    "```\n",
    "* Optional: You can add as many markdown cells as you like to explain the test case.\n",
    "\n",
    "This is how it works\n",
    "* From the cell with the function definition all code above the docstring, including the docstring, will be stored as prompt. Many prompts from many notebooks will be collected in one `jsonl` file. This notebook does that.\n",
    "* Given language models will be asked to complete the code by adding python code below which does what the docstring claims.\n",
    "* Afterwards, the generated code examples will be executed and the tests will be run to see if the results were correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72835d2a-3a74-4cfe-91a6-2349a342105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ff314f-fc13-4f95-987d-68803d568023",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_notebook_directory = './human-eval-bia/' # must end with /\n",
    "# Specify the filename to save the .jsonl file\n",
    "target_jsonl_filename = '../data/human-eval-bia.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c6c23d-464d-4540-a58c-09683e70bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_id': './human-eval-bia/label_processing_0.ipynb', 'prompt': 'import skimage\\n\\ndef remove_labels_on_edges(label_image):\\n    \"\"\"\\n    Takes a label_image and removes all objects which touch the image border.\\n    \"\"\"', 'canonical_solution': '\\n    return skimage.segmentation.clear_border(label_image)', 'entry_point': 'remove_labels_on_edges', 'test': 'def check(candidate):\\n    import numpy as np\\n    \\n    result = candidate(np.asarray([\\n        [0,0,0,0,0],\\n        [1,2,2,0,0],\\n        [1,2,2,0,0],\\n        [1,0,0,3,0],\\n        [0,0,0,4,0],\\n    ]))\\n\\n    # -1 becaue background counts\\n    assert len(np.unique(result)) - 1 == 2\\n    assert result.shape[0] == 5\\n    assert result.shape[1] == 5'}\n",
      "{'task_id': './human-eval-bia/label_processing_1.ipynb', 'prompt': 'import skimage\\n\\ndef label_sequentially(label_image):\\n    \"\"\"\\n    Takes a label_image with n labels and relabels the objects, \\n    to make sure all integer labels between 0 and n are used. \\n    No gaps are there.\\n    \"\"\"', 'canonical_solution': '\\n    return skimage.segmentation.relabel_sequential(label_image)[0]', 'entry_point': 'label_sequentially', 'test': 'def check(candidate):\\n    import numpy as np\\n    \\n    result = candidate(np.asarray([\\n        [0,1,0,0,0],\\n        [0,0,0,0,0],\\n        [0,4,0,0,0],\\n        [0,0,5,0,0],\\n        [0,0,0,6,0],\\n    ])) \\n\\n    # -1 becaue background counts\\n    assert len(np.unique(result)) - 1 == 4\\n    assert result.max() == 4\\n    assert result.shape[0] == 5\\n    assert result.shape[1] == 5'}\n",
      "{'task_id': './human-eval-bia/segmentation_0.ipynb', 'prompt': 'import skimage\\nimport numpy as np\\n\\ndef label_binary_image_and_count_labels(binary_image):\\n    \"\"\"\\n    Consumes as input a binary image, applies connected component labeling to it, \\n    counts the labeled objects and returns their count as single number.\\n    \"\"\"', 'canonical_solution': '\\n    label_image = skimage.measure.label(binary_image)\\n\\n    return len(np.unique(label_image)) - 1', 'entry_point': 'label_binary_image_and_count_labels', 'test': 'def check(candidate):\\n    assert candidate(np.asarray([\\n        [0,0,0,0,0],\\n        [0,1,0,0,0],\\n        [0,0,0,0,0],\\n        [1,0,0,0,0],\\n        [0,0,0,1,0],\\n    ])) == 3\\n\\n    assert candidate(np.asarray([\\n        [0,0,0,0,0],\\n        [0,1,0,0,0],\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n    ])) == 1\\n\\n    assert candidate(np.asarray([\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n    ])) == 0'}\n",
      "{'task_id': './human-eval-bia/segmentation_1.ipynb', 'prompt': 'import skimage\\nimport numpy as np\\n\\ndef apply_otsu_threshold_and_count_postiive_pixels(image):\\n    \"\"\"\\n    Takes an image, applies Otsu\\'s threshold method to it to create a binary image and \\n    counts the positive pixels.\\n    \"\"\"', 'canonical_solution': '\\n    binary_image = image > skimage.filters.threshold_otsu(image)\\n\\n    result = np.sum(binary_image)\\n    print( result )\\n    return result', 'entry_point': 'apply_otsu_threshold_and_count_postiive_pixels', 'test': 'def check(candidate):\\n    assert candidate(np.asarray([\\n        [0,0,0,0,0],\\n        [1,1,1,0,0],\\n        [1,1,1,0,0],\\n        [1,0,0,0,0],\\n        [0,0,0,1,0],\\n    ])) == 8\\n\\n    assert candidate(np.asarray([\\n        [0,0,0,0,0],\\n        [0,1,0,0,0],\\n        [1,2,1,0,0],\\n        [0,1,3,4,0],\\n        [0,1,4,1,0],\\n    ])) == 4\\n\\n    assert candidate(np.asarray([\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n        [0,0,0,0,0],\\n    ])) == 0'}\n"
     ]
    }
   ],
   "source": [
    "list_of_cases = []\n",
    "\n",
    "\n",
    "# List all files in the current directory\n",
    "files = os.listdir(source_notebook_directory)\n",
    "\n",
    "# Iterate through the files and print names ending with .ipynb\n",
    "for file in files:\n",
    "    if file.endswith('.ipynb'):\n",
    "        notebook_filename = source_notebook_directory + file\n",
    "        \n",
    "        # Load and parse the notebook\n",
    "        with open(notebook_filename, 'r') as file:\n",
    "            notebook = json.load(file)\n",
    "        \n",
    "        task_id = notebook_filename\n",
    "        prompt = None\n",
    "        canonical_solution = None\n",
    "        entry_point = None\n",
    "        test = None\n",
    "        \n",
    "        # Iterate through the cells and print the source of code cells\n",
    "        for cell in notebook['cells']:\n",
    "            if cell['cell_type'] == 'code':\n",
    "                # Joining the lines of code for better readability\n",
    "                code = ''.join(cell['source'])\n",
    "                # print('\\n\\nCODE\\n\\n',code)\n",
    "        \n",
    "                if code.startswith('check('):\n",
    "                    entry_point = code.strip().replace(\"check(\",\"\").replace(\")\",\"\").strip()\n",
    "                elif '\"\"\"' in code:\n",
    "                    temp = code.split('\"\"\"')\n",
    "                    canonical_solution = temp[-1]\n",
    "                    temp[-1] = \"\"\n",
    "                    prompt = '\"\"\"'.join(temp)\n",
    "                elif 'def check(' in code:\n",
    "                    test = code \n",
    "                elif len(code.strip()) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    sample = code[:20]\n",
    "                    warnings.warn(f\"I had issues reading a cell in {task_id} starting with \")\n",
    "                    \n",
    "        if prompt is None:\n",
    "            warnings.warn(f\"Couldn't extract prompt from {task_id}.\")\n",
    "        elif canonical_solution is None:\n",
    "            warnings.warn(f\"Couldn't extract canonical_solution from {task_id}.\")\n",
    "        elif entry_point is None:\n",
    "            warnings.warn(f\"Couldn't extract entry_point from {task_id}.\")\n",
    "        \n",
    "        test_case = {\n",
    "            'task_id':task_id,\n",
    "            'prompt':prompt,\n",
    "            'canonical_solution':canonical_solution,\n",
    "            'entry_point':entry_point,\n",
    "            'test':test\n",
    "        }\n",
    "\n",
    "        print(test_case)\n",
    "        list_of_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339ee8e9-457e-4d4f-a521-f758c36fb3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to ../data/human-eval-bia.jsonl.\n"
     ]
    }
   ],
   "source": [
    "# Open the file in write mode and save the dictionaries\n",
    "with open(target_jsonl_filename, 'w') as file:\n",
    "    for dictionary in list_of_cases:\n",
    "        # Convert dictionary to a JSON formatted string and write it\n",
    "        json_str = json.dumps(dictionary)\n",
    "        file.write(json_str + '\\n')\n",
    "\n",
    "print(f'Data successfully saved to {target_jsonl_filename}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8315490-aa34-44c5-b1e8-048270e4f14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62da25-c79d-437a-b586-7e139abfe1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
