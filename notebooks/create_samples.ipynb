{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad39f74d-59a7-452d-a58a-22733972468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbe6f3e-e85b-4505-a0f7-e11421082f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BLABLADOR_API_KEY'] = 'glpat-shzB7BvqtZGHW7bGBXSx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19eea9c-ed76-4ea1-b855-875578e562d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpt35 = False\n",
    "use_gpt4 = True\n",
    "use_ollama_codellama = False\n",
    "use_blablador_mistral = False\n",
    "use_gemini_pro = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03910e60-ea98-4d44-90aa-f487ee839510",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005dc6b0-8b18-4d4c-b7b4-dc59799c99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prompt(input_code):\n",
    "    prompt = f\"\"\"Complete the following code. Return the complete code including my code.\n",
    "\n",
    "```python\n",
    "{input_code}\n",
    "```\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824f29ac-4603-438c-8101-1ee0c0309994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python(response):\n",
    "    if '[PYTHON]' in response and '[/PYTHON]' in response:\n",
    "        response = response.replace('[PYTHON]', '```')\n",
    "        response = response.replace('[/PYTHON]', '```')\n",
    "        \n",
    "    if '```' in response:\n",
    "        response = response.replace('```python', '```')\n",
    "        \n",
    "        response = response.split('```')[-2]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936d83a-a405-4cab-9176-5c9379a153d1",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7e637e-1100-4fbd-98ee-8853976f7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generators = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6a33cd-0401-4a7f-a4ef-6f1b72a03c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt35:\n",
    "    model_gpt35 = \"gpt-3.5-turbo-1106\"\n",
    "    def generate_one_completion_gpt35(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt35,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return extract_python(response.choices[0].message.content.strip())\n",
    "\n",
    "    code_generators[model_gpt35] = generate_one_completion_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e92e34-c5f6-497c-8860-2664d302efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt4:\n",
    "    model_gpt4 = \"gpt-4-1106-preview\"\n",
    "    def generate_one_completion_gpt4(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt4,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return extract_python(response.choices[0].message.content.strip())\n",
    "\n",
    "    code_generators[model_gpt4] = generate_one_completion_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c228a55c-895e-43b9-8721-c4c62a93c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codellama:\n",
    "    model_ollama_codellama = \"codellama\"\n",
    "    def generate_one_completion_codellama(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = 'http://localhost:11434/v1'\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_codellama,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return extract_python(response.choices[0].message.content.strip())\n",
    "\n",
    "    code_generators[model_ollama_codellama] = generate_one_completion_codellama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924d9691-3c00-447e-98d6-8c6a7e8c862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_blablador_mistral:\n",
    "    model_blablador_mistral = \"Mistral-7B-Instruct-v0.2\"\n",
    "    def generate_one_completion_blablador_mistral(input_code):\n",
    "        import openai\n",
    "        import os\n",
    "\n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = 'https://helmholtz-blablador.fz-juelich.de:8000/v1'\n",
    "        client.api_key = os.environ.get('BLABLADOR_API_KEY')\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_blablador_mistral,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return extract_python(response.choices[0].message.content.strip())\n",
    "\n",
    "    code_generators[model_blablador_mistral] = generate_one_completion_blablador_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fda77d0-af78-4712-ac47-b82490c542c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gemini_pro:\n",
    "    model_gemini_pro = 'gemini-pro'\n",
    "    \n",
    "    def generate_one_completion_gemini_pro(input_code):\n",
    "        from vertexai.preview.generative_models import (\n",
    "            GenerationConfig,\n",
    "            GenerativeModel,\n",
    "            Image,\n",
    "            Part,\n",
    "            ChatSession,\n",
    "        )\n",
    "        gemini_model = GenerativeModel(model_gemini_pro)\n",
    "        client = gemini_model.start_chat()\n",
    "        response = client.send_message(setup_prompt(input_code)).text\n",
    "\n",
    "        return extract_python(response)\n",
    "\n",
    "    code_generators[model_gemini_pro] = generate_one_completion_gemini_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f9051-6e0c-4ccf-9e03-fa2b9af88589",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e14dd9c9-4c96-409d-8ab8-4af0f6738f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-1106-preview \n",
      "Hello, World!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, func in code_generators.items():\n",
    "    print(key, func(\"def print_hello_world():\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63a329-c2e7-4f04-9e89-399605c3963c",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d7ccad-0e34-4790-ad5e-7898c0b0223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-1106-preview ./human-eval-bia/hello_world.ipynb 0\n",
      "gpt-4-1106-preview ./human-eval-bia/label_processing_0.ipynb 0\n",
      "gpt-4-1106-preview ./human-eval-bia/label_processing_1.ipynb 0\n",
      "gpt-4-1106-preview ./human-eval-bia/measure_0.ipynb 0\n",
      "gpt-4-1106-preview ./human-eval-bia/measure_1.ipynb 0\n",
      "gpt-4-1106-preview ./human-eval-bia/segmentation_0.ipynb 0\n",
      "gpt-4-1106-preview ./human-eval-bia/segmentation_1.ipynb 0\n",
      "gpt-4-1106-preview ./human-eval-bia/hello_world.ipynb 1\n",
      "gpt-4-1106-preview ./human-eval-bia/label_processing_0.ipynb 1\n",
      "gpt-4-1106-preview ./human-eval-bia/label_processing_1.ipynb 1\n",
      "gpt-4-1106-preview ./human-eval-bia/measure_0.ipynb 1\n",
      "gpt-4-1106-preview ./human-eval-bia/measure_1.ipynb 1\n",
      "gpt-4-1106-preview ./human-eval-bia/segmentation_0.ipynb 1\n",
      "gpt-4-1106-preview ./human-eval-bia/segmentation_1.ipynb 1\n"
     ]
    }
   ],
   "source": [
    "problems = read_problems('../data/human-eval-bia.jsonl')\n",
    "num_samples_per_task = 2\n",
    "\n",
    "for model_name, generate_one_completion in code_generators.items():\n",
    "    samples = []\n",
    "\n",
    "    for i in range(num_samples_per_task):\n",
    "        for task_id in problems:\n",
    "            print(model_name, task_id, i)\n",
    "            samples.append(dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"])))\n",
    "    \n",
    "    write_jsonl(f\"samples_{model_name}.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b71358-71f4-4719-91f5-6e1608b94b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
