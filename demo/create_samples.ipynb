{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d329258-7ec0-413f-ba69-dc8b375fd4a6",
   "metadata": {},
   "source": [
    "# Create samples from LLMs / endpoints\n",
    "In this notebook we ask LLMs to produce samples using given prompts. Results are saved as \"samples_....json\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad39f74d-59a7-452d-a58a-22733972468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems, extract_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ee967e-c0ed-4327-93f4-1ba85e0139bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data/\"\n",
    "problem_file = 'human-eval-bia.jsonl'\n",
    "num_samples_per_task = 10\n",
    "ollama_base_url = \"http://127.0.0.1:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09d2a78-daf8-4bc4-8401-f91a142a6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not running OpenAI API, comment out the following line\n",
    "#import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"AACACA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19eea9c-ed76-4ea1-b855-875578e562d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reference = False\n",
    "use_gpt35 = False\n",
    "use_gpt4 = False\n",
    "use_gpt4_20240409 = False\n",
    "use_gpt_4o_2024_05_13 = False\n",
    "use_gpt_4o_2024_08_06 = False\n",
    "use_gpt_4omini_2024_07_18 = False\n",
    "use_blablador_mistral = False\n",
    "use_gemini_pro = False\n",
    "use_gemini_15_pro = False\n",
    "use_gemini_15_flash = False\n",
    "use_gemini_ultra = False\n",
    "use_claude = False\n",
    "use_claude_35_sonnet = False\n",
    "use_ollama_mixtral8x7b = False\n",
    "use_ollama_mixtral8x22b = False\n",
    "use_ollama_llama3 = False\n",
    "use_ollama_codegemma_instruct7b = False\n",
    "use_ollama_codegemma_code7b = False\n",
    "use_ollama_codegemma_code2b = False\n",
    "use_ollama_codellama_instruct70b = False\n",
    "use_ollama_codellama_code70b = False\n",
    "use_ollama_codellama_python70b = False\n",
    "use_ollama_command_r_plus = False\n",
    "use_ollama_phi3 = False\n",
    "use_ollama_wizardlm2 = False\n",
    "use_ollama_mistral_nemo = False\n",
    "use_deepseek_coder_v2_lite = False\n",
    "use_o1_preview_2024_09_12 = False\n",
    "use_o1_mini_2024_09_12 = False\n",
    "use_kisski_llama31_405b = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03910e60-ea98-4d44-90aa-f487ee839510",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005dc6b0-8b18-4d4c-b7b4-dc59799c99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prompt(input_code):\n",
    "    prompt = f\"\"\"Complete the following code. \n",
    "    First, write down a plan as comments how to solve the problem step-by-step.\n",
    "    Then, import the python libraries you think you will use.\n",
    "    Then, write the function you were asked for.\n",
    "    Write python code only.\n",
    "    Do NOT write any code for testing the function.\n",
    "    Return the complete code including my code.\n",
    "\n",
    "```python\n",
    "{input_code}\n",
    "```\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936d83a-a405-4cab-9176-5c9379a153d1",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7e637e-1100-4fbd-98ee-8853976f7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generators = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bc7ec5-4349-42ee-bd65-4d66fc24b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_reference:\n",
    "    # actually not a model, but to the evaluation framework it appears like:\n",
    "    model_reference = 'reference'\n",
    "    problems_data = read_problems(directory + problem_file)\n",
    "\n",
    "    def generate_reference(input_code):\n",
    "        # This is a computationally wasteful solution, \n",
    "        # but like this it fits well in the framework\n",
    "        for task_id, problem in problems_data.items():\n",
    "            if problem['prompt'] == input_code:\n",
    "                return problem['canonical_solution']\n",
    "    \n",
    "    code_generators[model_reference] = generate_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed16f34-19c4-4d97-88e5-6b867fd545f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_mistral_nemo:\n",
    "    model_ollama_mistral = \"mistral-nemo\"\n",
    "    def generate_one_completion_mistral_nemo(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_mistral,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_mistral] = generate_one_completion_mistral_nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9edcf542-7ef6-4211-97b0-f888bc9591a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_deepseek_coder_v2_lite:\n",
    "    model_ollama_deepseek_coder_v2_lite = \"deepseek-coder-v2\"\n",
    "    def generate_one_completion_deepseek_coder_v2_lite(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_deepseek_coder_v2_lite,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_deepseek_coder_v2_lite] = generate_one_completion_deepseek_coder_v2_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247e31fc-14ec-425f-97bb-bade1eb5f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_mixtral8x7b:\n",
    "    model_ollama_mixtral = \"mixtral:8x7b-instruct-v0.1-q5_0\"\n",
    "    def generate_one_completion_mixtral8x7b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_mixtral,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_mixtral] = generate_one_completion_mixtral8x7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b27d60-cb3c-4c5a-a542-ccd1b45f33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_mixtral8x22b:\n",
    "    model_ollama_mixtral = \"mixtral:8x22b-instruct-v0.1-q4_0\"\n",
    "    def generate_one_completion_mixtral8x22b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_mixtral,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_mixtral] = generate_one_completion_mixtral8x22b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01397524",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_llama3:\n",
    "    # model_ollama_llama3 = \"llama3:70b-instruct-q8_0\"\n",
    "    # model_ollama_llama3 = \"llama3:70b-instruct-q4_0\"\n",
    "    model_ollama_llama3 = \"llama3:8b-instruct-fp16\"\n",
    "    def generate_one_completion_llama3(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_llama3,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_llama3] = generate_one_completion_llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e24509d-2304-476a-b52e-115d7d4e0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_kisski_llama31_405b:\n",
    "    model_kisski_llama31_405b = \"meta-llama-3.1-405b-instruct\"\n",
    "    def generate_one_completion_llama31_405b(input_code):\n",
    "        import openai\n",
    "        import os\n",
    "\n",
    "        import time\n",
    "        time.sleep(5 * 60)\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = \"https://chat-ai.academiccloud.de/l1/\"\n",
    "        client.api_key = os.environ[\"KISSKI_API_KEY\"]\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_kisski_llama31_405b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_kisski_llama31_405b] = generate_one_completion_llama31_405b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8606a77-090b-44a8-a7ec-0f8bc09f64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codegemma_instruct7b:\n",
    "    model_ollama_codegemma_instruct7b = \"codegemma:7b-instruct-fp16\"\n",
    "    def generate_one_completion_codegemma_instruct7b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_codegemma_instruct7b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codegemma_instruct7b] = generate_one_completion_codegemma_instruct7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f106e487-e197-4cdd-b749-ede1895f38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codegemma_code7b:\n",
    "    model_ollama_codegemma_code7b = \"codegemma:7b-code-fp16\"\n",
    "    def generate_one_completion_codegemma_code7b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_codegemma_code7b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codegemma_code7b] = generate_one_completion_codegemma_code7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0f2a5e4-3b36-4fbe-8f15-8674dd6341b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codegemma_code2b:\n",
    "    model_ollama_codegemma_code2b = \"codegemma:2b-code-fp16\"\n",
    "    def generate_one_completion_codegemma_code2b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_codegemma_code2b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codegemma_code2b] = generate_one_completion_codegemma_code2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c228a55c-895e-43b9-8721-c4c62a93c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codellama_instruct70b:\n",
    "    #model_ollama_codellama_instruct70b = \"codellama:70b-instruct-q4_0\"\n",
    "    model_ollama_codellama_instruct70b = \"codellama:70b-instruct-q8_0\"\n",
    "    def generate_one_completion_codellama_instruct70b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_codellama_instruct70b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codellama_instruct70b] = generate_one_completion_codellama_instruct70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faa6023e-c431-48ca-aa59-91ffa3093694",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codellama_code70b:\n",
    "    model_ollama_codellama_code70b = \"codellama:70b-code-q4_0\"\n",
    "    def generate_one_completion_codellama_code70b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_codellama_code70b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codellama_code70b] = generate_one_completion_codellama_code70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a7e32c3-b9d3-43ac-962f-687d363f43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codellama_python70b:\n",
    "    model_ollama_codellama_python70b = \"codellama:70b-python-q4_0\"\n",
    "    def generate_one_completion_codellama_python70b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_codellama_python70b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codellama_python70b] = generate_one_completion_codellama_python70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3a88c17-34cb-45fb-bb83-dd376d751bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_command_r_plus:\n",
    "    model_ollama_command_r_plus = \"command-r-plus:104b-q4_0\"\n",
    "    def generate_one_completion_command_r_plus(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_command_r_plus,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_command_r_plus] = generate_one_completion_command_r_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a85bf418-568f-475e-890e-04f740f82ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_phi3:\n",
    "    model_ollama_phi3 = \"phi3:3.8b-mini-instruct-4k-fp16\"\n",
    "    def generate_one_completion_phi3(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_phi3,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_phi3] = generate_one_completion_phi3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fa58c2e-8c9e-41f9-ab4e-ce8424f39862",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_wizardlm2:\n",
    "    model_ollama_wizardlm2 = \"wizardlm2:8x22b-q4_0\"\n",
    "    def generate_one_completion_wizardlm2(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_wizardlm2,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_wizardlm2] = generate_one_completion_wizardlm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc6a33cd-0401-4a7f-a4ef-6f1b72a03c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt35:\n",
    "    model_gpt35 = \"gpt-3.5-turbo-1106\"\n",
    "    def generate_one_completion_gpt35(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt35,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_gpt35] = generate_one_completion_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e92e34-c5f6-497c-8860-2664d302efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt4_20240409:\n",
    "    model_gpt4_20240409 = \"gpt-4-turbo-2024-04-09\"\n",
    "    def generate_one_completion_gpt4_20240409(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt4_20240409,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_gpt4_20240409] = generate_one_completion_gpt4_20240409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6198e0cf-9015-47b6-b459-ba39a678d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt_4o_2024_05_13:\n",
    "    model_gpt_4o_2024_05_13 = \"gpt-4o-2024-05-13\"\n",
    "    def generate_one_completion_gpt_4o_2024_05_13(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt_4o_2024_05_13,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_gpt_4o_2024_05_13] = generate_one_completion_gpt_4o_2024_05_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15ee2985-d8dd-4e19-9f82-5eff60221512",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt_4o_2024_08_06:\n",
    "    model_gpt_4o_2024_08_06 = \"gpt-4o-2024-08-06\"\n",
    "    def generate_one_completion_gpt_4o_2024_08_06(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt_4o_2024_08_06,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_gpt_4o_2024_08_06] = generate_one_completion_gpt_4o_2024_08_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1029b78e-23bf-4a58-be27-de9ed224dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt_4omini_2024_07_18:\n",
    "    model_gpt_4omini_2024_07_18 = \"gpt-4o-mini-2024-07-18\"\n",
    "    def generate_one_completion_gpt_4omini_2024_07_18(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt_4omini_2024_07_18,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_gpt_4omini_2024_07_18] = generate_one_completion_gpt_4omini_2024_07_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf98338b-6535-43ec-99f7-936e0072585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt4:\n",
    "    model_gpt4 = \"gpt-4-1106-preview\"\n",
    "    def generate_one_completion_gpt4(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt4,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_gpt4] = generate_one_completion_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37ec56f1-1b83-4611-b949-6952950122aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_o1_preview_2024_09_12:\n",
    "    model_o1_preview_2024_09_12 = \"o1-preview-2024-09-12\"\n",
    "    def generate_one_completion_o1_preview_2024_09_12(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_o1_preview_2024_09_12,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_o1_preview_2024_09_12] = generate_one_completion_o1_preview_2024_09_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d855d191-3cec-4119-be2d-61f5670ea600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_o1_mini_2024_09_12:\n",
    "    model_o1_mini_2024_09_12 = \"o1-mini-2024-09-12\"\n",
    "    def generate_one_completion_o1_mini_2024_09_12(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_o1_mini_2024_09_12,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_o1_mini_2024_09_12] = generate_one_completion_o1_mini_2024_09_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "924d9691-3c00-447e-98d6-8c6a7e8c862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_blablador_mistral:\n",
    "    model_blablador_mistral = \"Mistral-7B-Instruct-v0.2\"\n",
    "    def generate_one_completion_blablador_mistral(input_code):\n",
    "        import openai\n",
    "        import os\n",
    "\n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = 'https://helmholtz-blablador.fz-juelich.de:8000/v1'\n",
    "        client.api_key = os.environ.get('BLABLADOR_API_KEY')\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_blablador_mistral,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_blablador_mistral] = generate_one_completion_blablador_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fda77d0-af78-4712-ac47-b82490c542c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gemini_pro:\n",
    "    model_gemini_pro = 'gemini-pro'\n",
    "    \n",
    "    def generate_one_completion_gemini_pro(input_code):\n",
    "        from vertexai.preview.generative_models import (\n",
    "            GenerationConfig,\n",
    "            GenerativeModel,\n",
    "            Image,\n",
    "            Part,\n",
    "            ChatSession,\n",
    "        )\n",
    "        gemini_model = GenerativeModel(model_gemini_pro)\n",
    "        client = gemini_model.start_chat()\n",
    "        response = client.send_message(setup_prompt(input_code)).text\n",
    "\n",
    "        return response\n",
    "\n",
    "    code_generators[model_gemini_pro] = generate_one_completion_gemini_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb342866-3ac8-4077-a2a6-6369493b57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gemini_15_pro:\n",
    "    model_gemini_15_pro = 'gemini-1.5-pro-001'\n",
    "\n",
    "    def generate_one_completion_gemini_15_pro(input_code):\n",
    "        from google import generativeai as genai\n",
    "        import os\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        client = genai.GenerativeModel(model_gemini_15_pro)\n",
    "        result = client.generate_content(setup_prompt(input_code))\n",
    "        return result.text\n",
    "        \n",
    "    code_generators[model_gemini_15_pro] = generate_one_completion_gemini_15_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85c9dab3-b444-4f4c-bcdb-7b0ec70a118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gemini_15_flash:\n",
    "    model_gemini_15_flash = 'gemini-1.5-flash-001'\n",
    "\n",
    "    def generate_one_completion_gemini_15_flash(input_code):\n",
    "        from google import generativeai as genai\n",
    "        import os\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "        import time\n",
    "        time.sleep(10)\n",
    "        \n",
    "        client = genai.GenerativeModel(model_gemini_15_flash)\n",
    "        result = client.generate_content(setup_prompt(input_code))\n",
    "        return result.text\n",
    "        \n",
    "    code_generators[model_gemini_15_flash] = generate_one_completion_gemini_15_flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4605995-2db8-4315-a11c-a3f5345d75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gemini_ultra:\n",
    "    model_gemini_ultra = 'gemini-ultra'\n",
    "\n",
    "    def generate_one_completion_gemini_ultra(input_code):\n",
    "        from google import generativeai as genai\n",
    "        import os\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        client = genai.GenerativeModel(model_gemini_ultra)\n",
    "        result = client.generate_content(setup_prompt(input_code))\n",
    "        return result.text\n",
    "        \n",
    "    code_generators[model_gemini_ultra] = generate_one_completion_gemini_ultra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2ebd867-e57f-4a18-9749-06e21c83edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_claude:\n",
    "    model_claude = \"claude-3-opus-20240229\"\n",
    "\n",
    "    def generate_one_completion_claude(input_code):\n",
    "        #import os\n",
    "        from anthropic import Anthropic\n",
    "        \n",
    "        client = Anthropic(\n",
    "            # This is the default and can be omitted\n",
    "            #api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    "        )\n",
    "        \n",
    "        message = client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": setup_prompt(input_code),\n",
    "                }\n",
    "            ],\n",
    "            model=model_claude,\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    code_generators[model_claude] = generate_one_completion_claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46f9cb4d-4dc2-49e0-b400-cd1980205d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_claude_35_sonnet:\n",
    "    model_claude_35_sonnet = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "    def generate_one_completion_claude_35_sonnet(input_code):\n",
    "        #import os\n",
    "        from anthropic import Anthropic\n",
    "        \n",
    "        import time\n",
    "        time.sleep(10)\n",
    "        \n",
    "        client = Anthropic(\n",
    "            # This is the default and can be omitted\n",
    "            #api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    "        )\n",
    "        \n",
    "        message = client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": setup_prompt(input_code),\n",
    "                }\n",
    "            ],\n",
    "            model=model_claude_35_sonnet,\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    code_generators[model_claude_35_sonnet] = generate_one_completion_claude_35_sonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f9051-6e0c-4ccf-9e03-fa2b9af88589",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e14dd9c9-4c96-409d-8ab8-4af0f6738f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama-3.1-405b-instruct ```python\n",
      "# Plan to solve the problem step-by-step\n",
      "# 1. The function should not take any arguments.\n",
      "# 2. The function should print \"Hello, World!\" to the console.\n",
      "# 3. No calculations or data transformations are required.\n",
      "\n",
      "# Import necessary python libraries\n",
      "# No libraries are required for this task\n",
      "\n",
      "# Define the function\n",
      "def print_hello_world():\n",
      "    \"\"\"\n",
      "    Prints \"Hello, World!\" to the console.\n",
      "    \"\"\"\n",
      "    print(\"Hello, World!\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "for key, func in code_generators.items():\n",
    "    print(key, func(\"def print_hello_world():\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63a329-c2e7-4f04-9e89-399605c3963c",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0d7ccad-0e34-4790-ad5e-7898c0b0223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama-3.1-405b-instruct ../test_cases/apply_otsu_threshold_and_count_postiive_pixels.ipynb 0\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Connection to model broke",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_id \u001b[38;5;129;01min\u001b[39;00m problems:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model_name, task_id, i)\n\u001b[1;32m---> 10\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_one_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     code \u001b[38;5;241m=\u001b[39m extract_python(response)\n\u001b[0;32m     13\u001b[0m     samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mdict\u001b[39m(task_id\u001b[38;5;241m=\u001b[39mtask_id, completion\u001b[38;5;241m=\u001b[39mcode, full_response\u001b[38;5;241m=\u001b[39mresponse))\n",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m, in \u001b[0;36mgenerate_one_completion_llama31_405b\u001b[1;34m(input_code)\u001b[0m\n\u001b[0;32m     11\u001b[0m client\u001b[38;5;241m.\u001b[39mbase_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://chat-ai.academiccloud.de/l1/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m client\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKISSKI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kisski_llama31_405b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msetup_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_code\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\resources\\chat\\completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1269\u001b[0m     )\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\heb4\\lib\\site-packages\\openai\\_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1050\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1054\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1055\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1059\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1060\u001b[0m )\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Connection to model broke"
     ]
    }
   ],
   "source": [
    "problems = read_problems(directory + problem_file)\n",
    "\n",
    "for model_name, generate_one_completion in code_generators.items():\n",
    "    samples = []\n",
    "\n",
    "    for i in range(num_samples_per_task):\n",
    "        for task_id in problems:\n",
    "            print(model_name, task_id, i)\n",
    "\n",
    "            response = generate_one_completion(problems[task_id][\"prompt\"])\n",
    "            code = extract_python(response)\n",
    "            \n",
    "            samples.append(dict(task_id=task_id, completion=code, full_response=response))\n",
    "    \n",
    "        write_jsonl(f\"{directory}samples_{model_name}.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e50006e-d906-4ceb-9115-e65b63deaf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
