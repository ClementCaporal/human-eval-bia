{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d329258-7ec0-413f-ba69-dc8b375fd4a6",
   "metadata": {},
   "source": [
    "# Create samples from LLMs / endpoints\n",
    "In this notebook we ask LLMs to produce samples using given prompts. Results are saved as \"samples_....json\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39f74d-59a7-452d-a58a-22733972468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee967e-c0ed-4327-93f4-1ba85e0139bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data/\"\n",
    "problem_file = 'human-eval-bia.jsonl'\n",
    "num_samples_per_task = 10\n",
    "ollama_base_url = \"http://10.11.6.248:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19eea9c-ed76-4ea1-b855-875578e562d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reference = True\n",
    "use_gpt35 = False\n",
    "use_gpt4 = False\n",
    "use_gpt4_20240409 = False\n",
    "use_blablador_mistral = False\n",
    "use_gemini_pro = False\n",
    "use_gemini_15_pro = False\n",
    "use_gemini_ultra = False\n",
    "use_claude = False\n",
    "use_ollama_mixtral8x7b = False\n",
    "use_ollama_mixtral8x22b = False\n",
    "use_ollama_llama3 = False\n",
    "use_ollama_codegemma_instruct7b = False\n",
    "use_ollama_codegemma_code7b = False\n",
    "use_ollama_codegemma_code2b = False\n",
    "use_ollama_codellama_instruct70b = False\n",
    "use_ollama_codellama_code70b = False\n",
    "use_ollama_codellama_python70b = False\n",
    "use_ollama_command_r_plus = False\n",
    "use_ollama_phi3 = False\n",
    "use_ollama_wizardlm2 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03910e60-ea98-4d44-90aa-f487ee839510",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005dc6b0-8b18-4d4c-b7b4-dc59799c99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prompt(input_code):\n",
    "    prompt = f\"\"\"Complete the following code. \n",
    "    First, write down a plan as comments how to solve the problem step-by-step.\n",
    "    Then, import the python libraries you think you will use.\n",
    "    Then, write the function you were asked for.\n",
    "    Write python code only.\n",
    "    Do NOT write any code for testing the function.\n",
    "    Return the complete code including my code.\n",
    "\n",
    "```python\n",
    "{input_code}\n",
    "```\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f29ac-4603-438c-8101-1ee0c0309994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python(response):\n",
    "\n",
    "    if '[PYTHON]' in response and '[/PYTHON]' in response:\n",
    "        response = response.replace('[PYTHON]', '```')\n",
    "        response = response.replace('[/PYTHON]', '```')\n",
    "        \n",
    "    if '```' in response:\n",
    "        response = response.replace('```python', '```')\n",
    "        temp = response.split('```')\n",
    "        for i in range(0, len(temp), 2):\n",
    "            temp[i] = \"\"\n",
    "        response = \"\".join(temp)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936d83a-a405-4cab-9176-5c9379a153d1",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e637e-1100-4fbd-98ee-8853976f7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generators = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc7ec5-4349-42ee-bd65-4d66fc24b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_reference:\n",
    "    # actually not a model, but to the evaluation framework it appears like:\n",
    "    model_reference = 'reference'\n",
    "    problems_data = read_problems(directory + problem_file)\n",
    "\n",
    "    def generate_reference(input_code):\n",
    "        # This is a computationally wasteful solution, \n",
    "        # but like this it fits well in the framework\n",
    "        for task_id, problem in problems_data.items():\n",
    "            if problem['prompt'] == input_code:\n",
    "                return problem['canonical_solution']\n",
    "    \n",
    "    code_generators[model_reference] = generate_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed16f34-19c4-4d97-88e5-6b867fd545f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_mixtral8x7b:\n",
    "    model_ollama_mixtral = \"mixtral:8x7b-instruct-v0.1-q5_0\"\n",
    "    def generate_one_completion_mixtral8x7b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_mixtral,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_mixtral] = generate_one_completion_mixtral8x7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b27d60-cb3c-4c5a-a542-ccd1b45f33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_mixtral8x22b:\n",
    "    model_ollama_mixtral = \"mixtral:8x22b-instruct-v0.1-q4_0\"\n",
    "    def generate_one_completion_mixtral8x22b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_mixtral,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_mixtral] = generate_one_completion_mixtral8x22b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01397524",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_llama3:\n",
    "    # model_ollama_llama3 = \"llama3:70b-instruct-q8_0\"\n",
    "    # model_ollama_llama3 = \"llama3:70b-instruct-q4_0\"\n",
    "    model_ollama_llama3 = \"llama3:8b-instruct-fp16\"\n",
    "    def generate_one_completion_llama3(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_llama3,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_llama3] = generate_one_completion_llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8606a77-090b-44a8-a7ec-0f8bc09f64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codegemma_instruct7b:\n",
    "    model_ollama_codegemma_instruct7b = \"codegemma:7b-instruct-fp16\"\n",
    "    def generate_one_completion_codegemma_instruct7b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_codegemma_instruct7b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codegemma_instruct7b] = generate_one_completion_codegemma_instruct7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106e487-e197-4cdd-b749-ede1895f38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codegemma_code7b:\n",
    "    model_ollama_codegemma_code7b = \"codegemma:7b-code-fp16\"\n",
    "    def generate_one_completion_codegemma_code7b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_codegemma_code7b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codegemma_code7b] = generate_one_completion_codegemma_code7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2a5e4-3b36-4fbe-8f15-8674dd6341b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codegemma_code2b:\n",
    "    model_ollama_codegemma_code2b = \"codegemma:2b-code-fp16\"\n",
    "    def generate_one_completion_codegemma_code2b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_ollama_codegemma_code2b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codegemma_code2b] = generate_one_completion_codegemma_code2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228a55c-895e-43b9-8721-c4c62a93c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codellama_instruct70b:\n",
    "    #model_ollama_codellama_instruct70b = \"codellama:70b-instruct-q4_0\"\n",
    "    model_ollama_codellama_instruct70b = \"codellama:70b-instruct-q8_0\"\n",
    "    def generate_one_completion_codellama_instruct70b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_codellama_instruct70b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codellama_instruct70b] = generate_one_completion_codellama_instruct70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6023e-c431-48ca-aa59-91ffa3093694",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codellama_code70b:\n",
    "    model_ollama_codellama_code70b = \"codellama:70b-code-q4_0\"\n",
    "    def generate_one_completion_codellama_code70b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_codellama_code70b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codellama_code70b] = generate_one_completion_codellama_code70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e32c3-b9d3-43ac-962f-687d363f43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_codellama_python70b:\n",
    "    model_ollama_codellama_python70b = \"codellama:70b-python-q4_0\"\n",
    "    def generate_one_completion_codellama_python70b(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_codellama_python70b,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_codellama_python70b] = generate_one_completion_codellama_python70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a88c17-34cb-45fb-bb83-dd376d751bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_command_r_plus:\n",
    "    model_ollama_command_r_plus = \"command-r-plus:104b-q4_0\"\n",
    "    def generate_one_completion_command_r_plus(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_command_r_plus,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_command_r_plus] = generate_one_completion_command_r_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bf418-568f-475e-890e-04f740f82ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_phi3:\n",
    "    model_ollama_phi3 = \"phi3:3.8b-mini-instruct-4k-fp16\"\n",
    "    def generate_one_completion_phi3(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_phi3,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_phi3] = generate_one_completion_phi3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa58c2e-8c9e-41f9-ab4e-ce8424f39862",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ollama_wizardlm2:\n",
    "    model_ollama_wizardlm2 = \"wizardlm2:8x22b-q4_0\"\n",
    "    def generate_one_completion_wizardlm2(input_code):\n",
    "        import openai\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = ollama_base_url\n",
    "        response = client.chat.completions.create(\n",
    "            model= model_ollama_wizardlm2,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_ollama_wizardlm2] = generate_one_completion_wizardlm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a33cd-0401-4a7f-a4ef-6f1b72a03c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt35:\n",
    "    model_gpt35 = \"gpt-3.5-turbo-1106\"\n",
    "    def generate_one_completion_gpt35(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt35,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_gpt35] = generate_one_completion_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e92e34-c5f6-497c-8860-2664d302efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt4_20240409:\n",
    "    model_gpt4_20240409 = \"gpt-4-turbo-2024-04-09\"\n",
    "    def generate_one_completion_gpt4_20240409(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt4_20240409,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_gpt4_20240409] = generate_one_completion_gpt4_20240409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98338b-6535-43ec-99f7-936e0072585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpt4:\n",
    "    model_gpt4 = \"gpt-4-1106-preview\"\n",
    "    def generate_one_completion_gpt4(input_code):\n",
    "        import openai\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_gpt4,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    code_generators[model_gpt4] = generate_one_completion_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d9691-3c00-447e-98d6-8c6a7e8c862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_blablador_mistral:\n",
    "    model_blablador_mistral = \"Mistral-7B-Instruct-v0.2\"\n",
    "    def generate_one_completion_blablador_mistral(input_code):\n",
    "        import openai\n",
    "        import os\n",
    "\n",
    "        client = openai.OpenAI()\n",
    "        client.base_url = 'https://helmholtz-blablador.fz-juelich.de:8000/v1'\n",
    "        client.api_key = os.environ.get('BLABLADOR_API_KEY')\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_blablador_mistral,\n",
    "            messages=[{\"role\": \"user\", \"content\": setup_prompt(input_code)}],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    code_generators[model_blablador_mistral] = generate_one_completion_blablador_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda77d0-af78-4712-ac47-b82490c542c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gemini_pro:\n",
    "    model_gemini_pro = 'gemini-pro'\n",
    "    \n",
    "    def generate_one_completion_gemini_pro(input_code):\n",
    "        from vertexai.preview.generative_models import (\n",
    "            GenerationConfig,\n",
    "            GenerativeModel,\n",
    "            Image,\n",
    "            Part,\n",
    "            ChatSession,\n",
    "        )\n",
    "        gemini_model = GenerativeModel(model_gemini_pro)\n",
    "        client = gemini_model.start_chat()\n",
    "        response = client.send_message(setup_prompt(input_code)).text\n",
    "\n",
    "        return response\n",
    "\n",
    "    code_generators[model_gemini_pro] = generate_one_completion_gemini_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb342866-3ac8-4077-a2a6-6369493b57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gemini_15_pro:\n",
    "    model_gemini_15_pro = 'gemini-1.5-pro-latest'\n",
    "\n",
    "    def generate_one_completion_gemini_15_pro(input_code):\n",
    "        from google import generativeai as genai\n",
    "        import os\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        client = genai.GenerativeModel(model_gemini_15_pro)\n",
    "        result = client.generate_content(setup_prompt(input_code))\n",
    "        return result.text\n",
    "        \n",
    "    code_generators[model_gemini_15_pro] = generate_one_completion_gemini_15_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4605995-2db8-4315-a11c-a3f5345d75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gemini_ultra:\n",
    "    model_gemini_ultra = 'gemini-ultra'\n",
    "\n",
    "    def generate_one_completion_gemini_ultra(input_code):\n",
    "        from google import generativeai as genai\n",
    "        import os\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        client = genai.GenerativeModel(model_gemini_ultra)\n",
    "        result = client.generate_content(setup_prompt(input_code))\n",
    "        return result.text\n",
    "        \n",
    "    code_generators[model_gemini_ultra] = generate_one_completion_gemini_ultra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebd867-e57f-4a18-9749-06e21c83edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_claude:\n",
    "    model_claude = \"claude-3-opus-20240229\"\n",
    "\n",
    "    def generate_one_completion_claude(input_code):\n",
    "        #import os\n",
    "        from anthropic import Anthropic\n",
    "        \n",
    "        client = Anthropic(\n",
    "            # This is the default and can be omitted\n",
    "            #api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    "        )\n",
    "        \n",
    "        message = client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": setup_prompt(input_code),\n",
    "                }\n",
    "            ],\n",
    "            model=model_claude,\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    code_generators[model_claude] = generate_one_completion_claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f9051-6e0c-4ccf-9e03-fa2b9af88589",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14dd9c9-4c96-409d-8ab8-4af0f6738f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, func in code_generators.items():\n",
    "    print(key, func(\"def print_hello_world():\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63a329-c2e7-4f04-9e89-399605c3963c",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7ccad-0e34-4790-ad5e-7898c0b0223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = read_problems(directory + problem_file)\n",
    "\n",
    "for model_name, generate_one_completion in code_generators.items():\n",
    "    samples = []\n",
    "\n",
    "    for i in range(num_samples_per_task):\n",
    "        for task_id in problems:\n",
    "            print(model_name, task_id, i)\n",
    "\n",
    "            response = generate_one_completion(problems[task_id][\"prompt\"])\n",
    "            code = extract_python(response)\n",
    "            \n",
    "            samples.append(dict(task_id=task_id, completion=code, full_response=response))\n",
    "    \n",
    "    write_jsonl(f\"{directory}samples_{model_name}.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b71358-71f4-4719-91f5-6e1608b94b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b07c95-8bff-4088-8e78-86d2b149ea88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
